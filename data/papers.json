[
  {
    "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
    "category": "Semantic Reasoning",
    "summary": "GigaBrain-0 is a VLA foundation model that utilizes world-model-generated data and embodied Chain-of-Thought supervision to achieve superior generalization and long-horizon reasoning for complex robotic tasks.",
    "key_concepts": [
      "Embodied Chain-of-Thought (CoT)",
      "World Model-Generated Data",
      "RGBD Input Modeling",
      "Long-horizon Planning",
      "Generalist Robots"
    ],
    "filename": "GigaBrain-0 A World Model-Powered Vision-Language-Action Model.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Semantic Reasoning/GigaBrain-0 A World Model-Powered Vision-Language-Action Model.pdf"
  },
  {
    "title": "FASTER: TOWARD EFFICIENT AUTOREGRESSIVE VISION LANGUAGE ACTION MODELING VIA NEURAL ACTION TOKENIZATION",
    "category": "Speed and Deployment",
    "summary": "This paper introduces FASTer, a framework that improves the inference speed and efficiency of autoregressive Vision-Language-Action (VLA) models through a learnable action tokenizer and block-wise autoregressive decoding.",
    "key_concepts": [
      "neural action tokenization",
      "block-wise autoregression",
      "inference efficiency"
    ],
    "filename": "FASTer Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/FASTer Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization.pdf"
  },
  {
    "title": "Real-Time Execution of Action Chunking Flow Policies",
    "category": "Speed and Deployment",
    "summary": "The paper introduces Real-Time Chunking (RTC), an inference-time algorithm that enables smooth, asynchronous execution of high-latency action chunking policies by treating the generation of subsequent chunks as an inpainting problem.",
    "key_concepts": [
      "action chunking",
      "inference latency",
      "asynchronous execution",
      "flow matching inpainting",
      "real-time control"
    ],
    "filename": "Real-Time Execution of Action Chunking Flow Policies.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/Real-Time Execution of Action Chunking Flow Policies.pdf"
  },
  {
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "category": "Speed and Deployment",
    "summary": "VLASH is a general asynchronous inference framework for Vision-Language-Action models that eliminates action stalls and reduces reaction latency by conditioning predictions on estimated future robot states through state roll-forward and temporal-offset fine-tuning.",
    "key_concepts": [
      "Asynchronous Inference",
      "Future-State-Awareness",
      "Temporal Misalignment",
      "State Roll-forward",
      "Action Quantization",
      "Temporal-Offset Augmentation"
    ],
    "filename": "VLASH Real-Time VLAs via Future-State-Aware Asynchronous Inference.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/VLASH Real-Time VLAs via Future-State-Aware Asynchronous Inference.pdf"
  },
  {
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "category": "Algorithmic Foundations",
    "summary": "SRPO introduces a self-referential reinforcement learning framework for VLA models that mitigates reward sparsity by using in-batch successful trajectories and latent world representations to provide dense, progress-wise rewards for failed attempts.",
    "key_concepts": [
      "Self-Referential Policy Optimization",
      "Vision-Language-Action (VLA) Models",
      "Latent World Representations",
      "Progress-wise Reward Modeling",
      "Reinforcement Learning Post-training"
    ],
    "filename": "SRPO Self-Referential Policy Optimization for Vision-Language-Action Models.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Algorithmic Foundations/SRPO Self-Referential Policy Optimization for Vision-Language-Action Models.pdf"
  },
  {
    "title": "DEAS: DETACHED VALUE LEARNING WITH ACTION SEQUENCE FOR SCALABLE OFFLINE RL",
    "category": "Systems and Scale | Algorithmic Foundations",
    "summary": "DEAS is an offline RL framework that leverages action sequences within an options framework and employs detached value learning to improve performance and stability on complex, long-horizon manipulation tasks.",
    "key_concepts": [
      "Action Sequences",
      "Detached Value Learning",
      "Offline Reinforcement Learning",
      "Options Framework (SMDP)",
      "Vision-Language-Action Models (VLAs)"
    ],
    "filename": "DEAS DEtached value learning with Action Sequence for Scalable Offline RL.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Systems and Scale | Algorithmic Foundations/DEAS DEtached value learning with Action Sequence for Scalable Offline RL.pdf"
  },
  {
    "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment",
    "category": "Speed and Deployment",
    "summary": "Evo-1 is a 0.77 billion parameter Vision-Language-Action model that achieves state-of-the-art robotic manipulation performance using an efficient cross-modulated diffusion transformer and a two-stage training paradigm designed for real-time deployment.",
    "key_concepts": [
      "Lightweight VLA model",
      "Cross-modulated diffusion transformer",
      "Two-stage training paradigm",
      "Semantic alignment preservation",
      "Real-time robotic control"
    ],
    "filename": "Evo-1 Lightweight Vision-Language-Action Model with Preserved Semantic Alignment.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/Evo-1 Lightweight Vision-Language-Action Model with Preserved Semantic Alignment.pdf"
  },
  {
    "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies",
    "category": "Systems and Scale",
    "summary": "HiMoE-VLA introduces a hierarchical mixture-of-experts architecture to effectively manage heterogeneity in large-scale robotic datasets, enabling robust generalization across diverse embodiments and action spaces.",
    "key_concepts": [
      "Hierarchical Mixture-of-Experts (HiMoE)",
      "Vision-Language-Action (VLA) models",
      "Cross-embodiment learning",
      "Action-Space Regularization",
      "Flow-matching"
    ],
    "filename": "HiMoE-VLA Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Systems and Scale/HiMoE-VLA Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies.pdf"
  },
  {
    "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking",
    "category": "Speed and Deployment",
    "summary": "The paper proposes replacing computationally expensive inference-time inpainting in real-time chunking with training-time action conditioning, which simulates inference delays during training to improve reactive robot control without additional inference overhead.",
    "key_concepts": [
      "Real-time chunking (RTC)",
      "Vision-language-action models (VLAs)",
      "Training-time action conditioning",
      "Inference delay simulation",
      "Flow matching",
      "Asynchronous execution"
    ],
    "filename": "Training-Time Action Conditioning for Efficient Real-Time Chunking.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/Training-Time Action Conditioning for Efficient Real-Time Chunking.pdf"
  }
]