[
  {
    "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
    "category": "Semantic Reasoning",
    "summary": "GigaBrain-0 is a VLA foundation model that utilizes world-model-generated data and embodied Chain-of-Thought supervision to achieve superior generalization and long-horizon reasoning for complex robotic tasks.",
    "key_concepts": [
      "Embodied Chain-of-Thought (CoT)",
      "World Model-Generated Data",
      "RGBD Input Modeling",
      "Long-horizon Planning",
      "Generalist Robots"
    ],
    "filename": "GigaBrain-0 A World Model-Powered Vision-Language-Action Model.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Semantic Reasoning/GigaBrain-0 A World Model-Powered Vision-Language-Action Model.pdf"
  },
  {
    "title": "FASTER: TOWARD EFFICIENT AUTOREGRESSIVE VISION LANGUAGE ACTION MODELING VIA NEURAL ACTION TOKENIZATION",
    "category": "Speed and Deployment",
    "summary": "This paper introduces FASTer, a framework that improves the inference speed and efficiency of autoregressive Vision-Language-Action (VLA) models through a learnable action tokenizer and block-wise autoregressive decoding.",
    "key_concepts": [
      "neural action tokenization",
      "block-wise autoregression",
      "inference efficiency"
    ],
    "filename": "FASTer Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/FASTer Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization.pdf"
  },
  {
    "title": "Real-Time Execution of Action Chunking Flow Policies",
    "category": "Speed and Deployment",
    "summary": "The paper introduces Real-Time Chunking (RTC), an inference-time algorithm that enables smooth, asynchronous execution of high-latency action chunking policies by treating the generation of subsequent chunks as an inpainting problem.",
    "key_concepts": [
      "action chunking",
      "inference latency",
      "asynchronous execution",
      "flow matching inpainting",
      "real-time control"
    ],
    "filename": "Real-Time Execution of Action Chunking Flow Policies.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/Real-Time Execution of Action Chunking Flow Policies.pdf"
  },
  {
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "category": "Speed and Deployment",
    "summary": "VLASH is a general asynchronous inference framework for Vision-Language-Action models that eliminates action stalls and reduces reaction latency by conditioning predictions on estimated future robot states through state roll-forward and temporal-offset fine-tuning.",
    "key_concepts": [
      "Asynchronous Inference",
      "Future-State-Awareness",
      "Temporal Misalignment",
      "State Roll-forward",
      "Action Quantization",
      "Temporal-Offset Augmentation"
    ],
    "filename": "VLASH Real-Time VLAs via Future-State-Aware Asynchronous Inference.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/VLASH Real-Time VLAs via Future-State-Aware Asynchronous Inference.pdf"
  },
  {
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "category": "Algorithmic Foundations",
    "summary": "SRPO introduces a self-referential reinforcement learning framework for VLA models that mitigates reward sparsity by using in-batch successful trajectories and latent world representations to provide dense, progress-wise rewards for failed attempts.",
    "key_concepts": [
      "Self-Referential Policy Optimization",
      "Vision-Language-Action (VLA) Models",
      "Latent World Representations",
      "Progress-wise Reward Modeling",
      "Reinforcement Learning Post-training"
    ],
    "filename": "SRPO Self-Referential Policy Optimization for Vision-Language-Action Models.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Algorithmic Foundations/SRPO Self-Referential Policy Optimization for Vision-Language-Action Models.pdf"
  },
  {
    "title": "DEAS: DETACHED VALUE LEARNING WITH ACTION SEQUENCE FOR SCALABLE OFFLINE RL",
    "category": "Systems and Scale | Algorithmic Foundations",
    "summary": "DEAS is an offline RL framework that leverages action sequences within an options framework and employs detached value learning to improve performance and stability on complex, long-horizon manipulation tasks.",
    "key_concepts": [
      "Action Sequences",
      "Detached Value Learning",
      "Offline Reinforcement Learning",
      "Options Framework (SMDP)",
      "Vision-Language-Action Models (VLAs)"
    ],
    "filename": "DEAS DEtached value learning with Action Sequence for Scalable Offline RL.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Systems and Scale | Algorithmic Foundations/DEAS DEtached value learning with Action Sequence for Scalable Offline RL.pdf"
  },
  {
    "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment",
    "category": "Speed and Deployment",
    "summary": "Evo-1 is a 0.77 billion parameter Vision-Language-Action model that achieves state-of-the-art robotic manipulation performance using an efficient cross-modulated diffusion transformer and a two-stage training paradigm designed for real-time deployment.",
    "key_concepts": [
      "Lightweight VLA model",
      "Cross-modulated diffusion transformer",
      "Two-stage training paradigm",
      "Semantic alignment preservation",
      "Real-time robotic control"
    ],
    "filename": "Evo-1 Lightweight Vision-Language-Action Model with Preserved Semantic Alignment.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/Evo-1 Lightweight Vision-Language-Action Model with Preserved Semantic Alignment.pdf"
  },
  {
    "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies",
    "category": "Systems and Scale",
    "summary": "HiMoE-VLA introduces a hierarchical mixture-of-experts architecture to effectively manage heterogeneity in large-scale robotic datasets, enabling robust generalization across diverse embodiments and action spaces.",
    "key_concepts": [
      "Hierarchical Mixture-of-Experts (HiMoE)",
      "Vision-Language-Action (VLA) models",
      "Cross-embodiment learning",
      "Action-Space Regularization",
      "Flow-matching"
    ],
    "filename": "HiMoE-VLA Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Systems and Scale/HiMoE-VLA Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies.pdf"
  },
  {
    "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking",
    "category": "Speed and Deployment",
    "summary": "The paper proposes replacing computationally expensive inference-time inpainting in real-time chunking with training-time action conditioning, which simulates inference delays during training to improve reactive robot control without additional inference overhead.",
    "key_concepts": [
      "Real-time chunking (RTC)",
      "Vision-language-action models (VLAs)",
      "Training-time action conditioning",
      "Inference delay simulation",
      "Flow matching",
      "Asynchronous execution"
    ],
    "filename": "Training-Time Action Conditioning for Efficient Real-Time Chunking.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed and Deployment/Training-Time Action Conditioning for Efficient Real-Time Chunking.pdf"
  },
  {
    "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations",
    "category": "Systems and Scale",
    "summary": "ViVLA is a generalist robotic manipulation policy that enables one-shot task learning from a single expert video demonstration by employing a latent action tokenizer with cycle-consistency and a scalable data generation pipeline that produces nearly 900,000 expert-agent pairs.",
    "key_concepts": [
      "One-Shot Visual Imitation Learning",
      "Vision-Language-Action (VLA) Model",
      "Action-Centric Cycle-Consistency (A3C)",
      "Parallel Decoding",
      "Cross-embodiment Transfer"
    ],
    "filename": "See Once, Then Act Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Systems and Scale/See Once, Then Act Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations.pdf"
  },
  {
    "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation",
    "category": "Robustness and Reliability",
    "summary": "The paper introduces Affordance Field Intervention (AFI), a hybrid framework that integrates 3D Spatial Affordance Fields into Vision-Language-Action (VLA) models to detect and escape 'memory traps' by providing geometric guidance during out-of-distribution tasks.",
    "key_concepts": [
      "Memory Trap",
      "Vision-Language-Action (VLA) Models",
      "3D Spatial Affordance Fields (SAFs)",
      "Out-of-Distribution (OOD) Generalization",
      "Trajectory Rollback"
    ],
    "filename": "Affordance Field Intervention Enabling VLAs to Escape Memory Traps in Robotic Manipulation.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Robustness and Reliability/Affordance Field Intervention Enabling VLAs to Escape Memory Traps in Robotic Manipulation.pdf"
  },
  {
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "category": "Systems and Scale",
    "summary": "VideoVLA transforms large-scale pre-trained video generation models into robotic manipulators by jointly predicting actions and future visual outcomes, significantly enhancing generalization to novel objects and cross-embodiment skills.",
    "key_concepts": [
      "Video Diffusion Transformer",
      "Vision-Language-Action (VLA)",
      "Dual-prediction strategy",
      "Cross-embodiment generalization",
      "Visual imagination"
    ],
    "filename": "VideoVLA Video Generators Can Be Generalizable Robot Manipulators.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Systems and Scale/VideoVLA Video Generators Can Be Generalizable Robot Manipulators.pdf"
  },
  {
    "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
    "category": "Semantic Reasoning (The \"Thinkers\")",
    "bottleneck": "Complexity",
    "justification": "The paper addresses the 'temporal myopia' of Markovian VLA models by introducing a bidirectional reasoning framework. It moves beyond simple reactive policies by implementing a 'think-while-acting' paradigm where the model explicitly reasons about past dynamics (hindsight) and future motion trajectories (foresight) to maintain coherence in long-horizon tasks. This focus on structured temporal context and goal-directed behavior rather than purely reactive responses aligns with the 'Thinkers' category's emphasis on high-level reasoning and planning.",
    "evidence": "\"think-while-acting,\" \"bidirectional temporal reasoning,\" \"temporal myopia,\" \"long-horizon coherence,\" \"foresight reasoning,\" \"hindsight-modulated joint expert.\"",
    "key_concepts": [
      "Motion Vectors (MVs)",
      "Hindsight Priors",
      "Foresight Reasoning",
      "Long-horizon Coherence",
      "Bidirectional Temporal Context"
    ],
    "filename": "HiF-VLA Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Semantic Reasoning/HiF-VLA Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models.pdf"
  },
  {
    "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
    "category": "Speed & Deployment (The \"Fast Movers\")",
    "bottleneck": "Latency",
    "justification": "The paper focuses on the 'Latency Wall' by addressing the computational overhead of large-scale VLA models during real-time inference. It introduces TEAM-VLA, a framework designed to accelerate inference speed through token pruning and merging without requiring retraining or parameter updates. By reducing the number of visual tokens processed by the Transformer backbone, it achieves a 1.5x speedup, making it more practical for high-frequency, closed-loop robotic control.",
    "evidence": "\"accelerates VLA inference,\" \"real-time deployment,\" \"1.5x speedup,\" \"CUDA latency,\" \"computational efficiency,\" \"low-latency control scenarios,\" and \"on-device robotics.\"",
    "key_concepts": [
      "Token Compression",
      "Inference Acceleration",
      "Training-Free",
      "Bipartite Merging",
      "Token Pruning"
    ],
    "filename": "Token Expand-Merge Training-Free Token Compression for Vision-Language-Action Models.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Speed & Deployment/Token Expand-Merge Training-Free Token Compression for Vision-Language-Action Models.pdf"
  },
  {
    "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models",
    "category": "Robustness & Reliability (The \"Shields\")",
    "bottleneck": "Generalization",
    "justification": "GLaD is primarily concerned with overcoming the 'memorization' trap common in VLA models, where policies rely on superficial 2D semantic patterns (CLIP/SigLIP) and fail under visual perturbations. By distilling 3D geometric priors into the LLM hidden states, the model achieves significantly higher robustness to out-of-distribution (OOD) visual variations\u2014such as changes in object color, texture, and size\u2014while maintaining the same geometric structure. It specifically uses the LIBERO-PRO robustness benchmark to prove that geometric awareness acts as a 'shield' against the performance collapses typically seen in messy or visually shifted environments.",
    "evidence": "\"substantial robustness to visual appearance variations,\" \"LIBERO-PRO robustness benchmark,\" \"object perturbations (color, texture, size),\" \"genuine task understanding versus mere memorization,\" \"policy generalization beyond pattern matching.\"",
    "key_concepts": [
      "Knowledge Distillation",
      "3D Geometric Priors",
      "Visual Perturbation Robustness",
      "Spatial Reasoning"
    ],
    "filename": "GLaD Geometric Latent Distillation for Vision-Language-Action Models.pdf",
    "github_link": "https://github.com/ai-sales-agent-25/VLA-RL-Papers/blob/main/papers/Robustness & Reliability/GLaD Geometric Latent Distillation for Vision-Language-Action Models.pdf"
  }
]